{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/banner.jpeg)\n",
    "Photo by <a href=\"https://unsplash.com/@towfiqu999999?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Towfiqu barbhuiya</a> on <a href=\"https://unsplash.com/s/photos/customer-ratings?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>\n",
    "  \n",
    "# I HATE this Product! Five Stars!\n",
    "### Reducing Rating Inflation Using NLP\n",
    "Author: Tom Chapman | [email](mailto:thomas.h.chapman@gmail.com) | [linkedin](https://www.linkedin.com/in/thomashchapman/) | [github](https://github.com/ThomasHChapman)\n",
    "\n",
    "Most online marketplaces use 5-star product rating systems which are prone to rating inflation, obscuring product quality and customer sentiment. In addition to being problematic for the consumer, rating inflation makes it difficult for sellers to parse customer feedback and understand how their products are being received by the public. My project uses natural language processing to train a model on review sentiment so that it can be used to accurately classify customer reviews based on their content.\n",
    "\n",
    "\n",
    "## Business Understanding\n",
    "Since their inception, online marketplaces have fundamentally shifted how consumers shop. With a near limitless number of products and services available online, it has never been easier to avoid the hassle of brick-and-mortar stores. However, identifying which products or services are high-quality has become increasingly difficult. The popularity of the 5-star rating scale has led to a number of well-documented challenges. [Harvard Business Review](https://hbr.org/2019/07/the-problems-with-5-star-rating-systems-and-how-to-fix-them) summed these challenges up nicely as follows:\n",
    "\n",
    "- There is little incentive for consumers to provide truthful feedback, meaning that extreme experiences (whether positive or negative) are much more likely to lead a consumer to leave a review.\n",
    "- Compounding the lack of incentive for truth, 5-star rating scales are prone to grade inflation. There is no correlation between the star-rating and the sentiment the user expresses in a review. It's possible (and surprisingly common) for a user to hate a product, excoriate it in a review, and then rate it 5-stars. This leads to inflated ratings, and makes it harder for the consumer to understand the meaning behind varied product ratings. How much better is a product with a 4.7 star rating than a 4.5 star average rating?\n",
    "My model is intended to help address rating inflation by classifying user feedback as positive or negative based on its content.\n",
    "\n",
    "Amazon is the largest online marketplace currently in existence, and its challenges with rating inflation are [common knowledge](https://www.nytimes.com/2021/06/18/technology/amazon-reviews.html). However, any marketplace that uses a five-star rating system (Google Play, Apple App Store, Chewy, Wal-mart, Etsy, Rakuten, etc.) can utilize my model to reclassify user reviews into positive or negative polarity. Offering customers a clearer display of product quality allows customers to make more informed purchases, and should drive improved satisfaction. It should also help mitigate poor quality or scam sellers inherently, as poor reviews are less likely to be drowned out by inflated ratings.\n",
    "\n",
    "The tool is also useful for sellers that want to move away from Amazon or implement their own storefront. By implementing my algorithm in a newly-created storefront, sellers can mostly automate the classification of consumer feedback and derive a more accurate understanding of how their products or services are being received. On average, Amazon sellers pay the website about 15% of their sale price on each item sold. There's no question the visibility and customer reach that Amazon provides is valuable, but 15% is an extremely hefty cost for small companies. For certain product categories, the cost per item can range as high as 45%, an enormous amount to pay simply for the storefront component of the site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Because of the enormity of the dataset, all substantive data cleaning was performed in [the cleaning notebook](preprocessing_nb.ipynb). The data preparation section in this notebook will be limited to importing necessary packages, reading in our pre-cleaned data, and dropping any null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import plot_confusion_matrix, confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import ParameterGrid, GridSearchCV, cross_validate, cross_val_score\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from imblearn.pipeline import Pipeline as ImPipeline\n",
    "\n",
    "import pickle\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data & preview its contents\n",
    "df = pd.read_csv('../data/train_cleaned_10sw.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review target variable distribution\n",
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find & Display any NaN values for cleaned text \n",
    "print(df.isna().sum())\n",
    "df[df['text_cleaned'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing text_cleaned values\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save off feature combinations for use in model iteration\n",
    "X = df['text_cleaned']\n",
    "\n",
    "X_bigrams = df[['text_cleaned', 'bigrams']]\n",
    "X_trigrams = df[['text_cleaned', 'trigrams']]\n",
    "X_allgrams = df[['text_cleaned', 'bigrams', 'trigrams']]\n",
    "\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "I utilized a number of sklearn's modeling techniques for the project. As noted in the data-preparation description, I experimented with the inclusion of bi-grams and tri-grams created from user input during the modeling process as well. I evaluated each model based on its accuracy score, as the real-world impact of a false positive or negative is quite low. With basic python it is easy to collect all the reviews that the classifier scores mislables for human review. Ultimately, a logistic regression classifier with default parameters was the best performer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Classifier\n",
    "To begin modelling efforts, I built a dummy classifier that simply guesses the most common class every time. Given that our training data is almost (but not perfectly) balanced, we should expect it to predict that every review is negative since there are slightly more negative reviews than positive reviews. If we can't beat a 50-50 guess with a dataset this large, something is horribly wrong. Predictably, the dummy classifier achieved an accuracy score of almost exactly 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Dummy Classifier pipeline\n",
    "dummy_pipe = ImPipeline(steps=[\n",
    "                                ('vect', TfidfVectorizer()),\n",
    "                                ('cf', DummyClassifier(strategy='most_frequent',\n",
    "                                                           random_state=42))\n",
    "                              ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit dummy pipe on the training data and plot confusion matrix\n",
    "dummy_pipe.fit(X, y)\n",
    "dummy_yhat = dummy_pipe.predict(X)\n",
    "plot_confusion_matrix(dummy_pipe, X, y, normalize='true');\n",
    "print(accuracy_score(y, dummy_yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes\n",
    "MNB is a commonly-used algorithm for natural language processing due to effectiveness in classifying topics while maintaining a low training time and relative simplicity. It predicts the probability that a given document belongs to a particular class based on the words it contains. I employed a multinomial naive bayes classifier as my first simple model for these reasons, and it will be the baseline model that I attempt to beat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the Multinomial Naieve Bayes Pipeline\n",
    "mnb_pipe = ImPipeline(steps=[\n",
    "                            ('vect', TfidfVectorizer()),\n",
    "                            ('cf', MultinomialNB())\n",
    "                            ]\n",
    ")\n",
    "\n",
    "# Fit MNB pipe on the training data, get predictions and plot confusion matrix\n",
    "mnb_pipe.fit(X, y)\n",
    "mnb_yhat = mnb_pipe.predict(X)\n",
    "plot_confusion_matrix(mnb_pipe, X, y, normalize='true');\n",
    "print(accuracy_score(y, mnb_yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtained cross-validated accuracy score\n",
    "mnb_cv = cross_validate(mnb_pipe, X, y)\n",
    "print(mnb_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'fit_time': 63.91176295, 64.27754903, 63.07658076, 62.96271181, 63.14784789\n",
    "- 'score_time': 14.80177712, 14.56706309, 14.5999763 , 14.49988723, 14.33135796\n",
    "- 'test_score': 0.81850038, 0.81476842, 0.81272648, 0.81008618, 0.81112924"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_cv['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model_performance Dictionary for use in visualizations, add mean model result.\n",
    "mod_perf = {}\n",
    "mod_perf['mnb_1grams'] = mnb_cv['test_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using N-grams as Features\n",
    "To see if I could improve on the model's performance over using single words, I tried giving it bi-grams and tri-grams as additional features. Ultimately, the MNB classifier was most accurate using only single words from each review as its inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Single Words & Bi-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Instantiate the MNB Pipeline with cleaned text and bi-grams\n",
    "mnb_bigrams_pipe = ImPipeline(steps=[\n",
    "            ('CT', ColumnTransformer([(x, TfidfVectorizer(), x) for x in ['text_cleaned',\n",
    "                                                                          'bigrams']\n",
    "                                     ])),\n",
    "            ('cf', MultinomialNB())\n",
    "]\n",
    ")\n",
    "\n",
    "# Fit MNB pipe on the cleaned text and bi-grams, get predictions, plot confusion matrix\n",
    "mnb_bigrams_pipe.fit(X_bigrams, y)\n",
    "mnb_bigrams_yhat = mnb_bigrams_pipe.predict(X_bigrams)\n",
    "plot_confusion_matrix(mnb_bigrams_pipe, X_bigrams, y, normalize='true');\n",
    "print(accuracy_score(y, mnb_bigrams_yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtained cross-validated accuracy score\n",
    "mnb_bigrams_cv = cross_validate(mnb_bigrams_pipe, X_bigrams, y)\n",
    "print(mnb_bigrams_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print cross-validated mean accuracy score & add to model performance dictionary\n",
    "print(mnb_bigrams_cv['test_score'].mean())\n",
    "mod_perf['mnb_2grams'] = mnb_bigrams_cv['test_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Single Words & Tri-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Multinomial Naive Bayes Pipeline with clean text and tri-grams\n",
    "mnb_trigrams_pipe = ImPipeline(steps=[\n",
    "                ('CT', ColumnTransformer([(x, TfidfVectorizer(), x) for x in ['text_cleaned',\n",
    "                                                                              'trigrams']\n",
    "                                         ])),\n",
    "                ('cf', MultinomialNB())\n",
    "]\n",
    ")\n",
    "\n",
    "# Fit MNB pipe on the training data, get predictions and plot confusion matrix\n",
    "mnb_trigrams_pipe.fit(X_trigrams, y)\n",
    "mnb_trigrams_yhat = mnb_trigrams_pipe.predict(X_trigrams)\n",
    "plot_confusion_matrix(mnb_trigrams_pipe, X_trigrams, y, normalize='true');\n",
    "print(accuracy_score(y, mnb_trigrams_yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtained cross-validated accuracy score\n",
    "mnb_trigrams_cv = cross_validate(mnb_trigrams_pipe, X_trigrams, y)\n",
    "print(mnb_trigrams_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print cross-validated mean accuracy score & add to model performance dictionary\n",
    "print(mnb_trigrams_cv['test_score'].mean())\n",
    "mod_perf['mnb_3grams'] = mnb_trigrams_cv['test_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Single Words, Bi-Grams & Tri-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Multinomial Naieve Bayes pipeline with clean text, bi-grams and tri-grams\n",
    "mnb_allgrams_pipe = ImPipeline(steps=[          \n",
    "                ('CT', ColumnTransformer([(x, TfidfVectorizer(), x) for x in ['text_cleaned',\n",
    "                                                                              'bigrams',\n",
    "                                                                              'trigrams']\n",
    "                                         ])),\n",
    "                ('cf', MultinomialNB())\n",
    "]\n",
    ")\n",
    "\n",
    "# Fit MNB pipe on the training data, get predictions and plot confusion matrix\n",
    "mnb_allgrams_pipe.fit(X_allgrams, y)\n",
    "mnb_allgrams_yhat = mnb_allgrams_pipe.predict(X_allgrams)\n",
    "plot_confusion_matrix(mnb_allgrams_pipe, X_allgrams, y, normalize='true');\n",
    "print(accuracy_score(y, mnb_allgrams_yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtained cross-validated accuracy score\n",
    "mnb_allgrams_cv = cross_validate(mnb_allgrams_pipe, X_allgrams, y)\n",
    "print(mnb_allgrams_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print cross-validated mean accuracy score & add to model performance dictionary\n",
    "print(mnb_allgrams_cv['test_score'].mean())\n",
    "mod_perf['mnb_allgrams'] = mnb_allgrams_cv['test_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Logistic Regression is a common step in any binary classification problem, so I elected to use it as my next modeling approach. It is a linear classification method that learns the probability of an observation belonging to a target class, and then finds the decision boundary that best separates the classes. It splits feature space linearly and typically works well even when there is correlation among the variables, as is the case in our dataset. Because the data is made up of consumer reviews, it is likely that similar language patterns may be observed throughout.\n",
    "\n",
    "Right away, we see a significant (5.6%) improvement in accuracy over our best-performing Naive Bayes model. By including tri-grams of our corpus along with single words, we see a performance gain of 5.8% over our best Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Logistic Regression Pipeline\n",
    "logreg_pipe = ImPipeline(steps=[\n",
    "                                 ('vect', TfidfVectorizer()),                              \n",
    "                                 ('lr', LogisticRegression(n_jobs=-2))\n",
    "                                ]\n",
    ")\n",
    "\n",
    "\n",
    "# Fit Logistic Regression pipeline on the training data, get predictions and plot confusion matrix\n",
    "logreg_pipe.fit(X, y)\n",
    "logreg_yhat = logreg_pipe.predict(X)\n",
    "plot_confusion_matrix(logreg_pipe, X, y, normalize='true');\n",
    "print(accuracy_score(y, logreg_yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtained cross-validated accuracy score\n",
    "logreg_cv = cross_validate(logreg_pipe, X, y)\n",
    "print(logreg_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print cross-validated mean accuracy score & add to model performance dictionary\n",
    "print(logreg_cv['test_score'].mean())\n",
    "mod_perf['logreg_1grams'] = logreg_cv['test_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using N-grams as Features\n",
    "To see if I could improve on the model's performance over using single words, I tried giving it bi-grams and tri-grams as additional features. This time, the combination of single words and tri-grams proved to be the best performer of the group, with a cross-validated accuracy score of 87.14%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Single Words & Bi-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiated the Logistic Regression Pipeline with clean text and bi-grams\n",
    "logreg_bigram_pipe = ImPipeline(steps=[\n",
    "                ('CT', ColumnTransformer([(x, TfidfVectorizer(), x) for x in ['text_cleaned',\n",
    "                                                                              'bigrams']\n",
    "                                         ])),                             \n",
    "                ('lr', LogisticRegression(n_jobs=-2))\n",
    "]\n",
    ")\n",
    "\n",
    "\n",
    "# Fit Logistic Regression pipeline on the training data, get predictions and plot confusion matrix\n",
    "logreg_bigram_pipe.fit(X_bigrams, y)\n",
    "logreg_bigram_yhat = logreg_bigram_pipe.predict(X_bigrams)\n",
    "plot_confusion_matrix(logreg_bigram_pipe, X_bigrams, y, normalize='true');\n",
    "print(accuracy_score(y, logreg_bigram_yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtained cross-validated accuracy score\n",
    "logreg_bigram_cv = cross_validate(logreg_bigram_pipe, X_bigrams, y)\n",
    "print(logreg_bigram_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print cross-validated mean accuracy score & add to model performance dictionary\n",
    "print(logreg_bigram_cv['test_score'].mean())\n",
    "mod_perf['logreg_2grams'] = logreg_bigram_cv['test_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Single Words & Tri-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Logistic Regression Pipeline with clean text and tri-grams\n",
    "logreg_trigram_pipe = ImPipeline(steps=[\n",
    "                ('CT', ColumnTransformer([(x, TfidfVectorizer(), x) for x in ['text_cleaned',\n",
    "                                                                              'trigrams']\n",
    "                                         ])),                             \n",
    "                ('lr', LogisticRegression(n_jobs=-2))\n",
    "]\n",
    ")\n",
    "\n",
    "\n",
    "# Fit LogReg pipeline on the training data, get predictions and plot confusion matrix\n",
    "logreg_trigram_pipe.fit(X_trigrams, y)\n",
    "logreg_trigram_yhat = logreg_trigram_pipe.predict(X_trigrams)\n",
    "plot_confusion_matrix(logreg_trigram_pipe, X_trigrams, y, normalize='true');\n",
    "print(accuracy_score(y, logreg_trigram_yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtained cross-validated accuracy score\n",
    "logreg_trigram_cv = cross_validate(logreg_trigram_pipe, X_trigrams, y)\n",
    "print(logreg_trigram_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print cross-validated mean accuracy score & add to model performance dictionary\n",
    "print(logreg_trigram_cv['test_score'].mean())\n",
    "mod_perf['logreg_3grams'] = logreg_trigram_cv['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model for future use\n",
    "with open('../models/sw_logreg_trigram_model.pkl', 'wb') as files:\n",
    "    joblib.dump(logreg_trigram_pipe, files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Single Words, Bi-Grams & Tri-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the LR Pipeline with clean text, bi-grams and tri-grams\n",
    "logreg_allgram_pipe = ImPipeline(steps=[\n",
    "                ('CT', ColumnTransformer([(x, TfidfVectorizer(), x) for x in ['text_cleaned',\n",
    "                                                                              'bigrams',\n",
    "                                                                              'trigrams']\n",
    "                                         ])),                             \n",
    "                ('lr', LogisticRegression(n_jobs=-2))\n",
    "]\n",
    ")\n",
    "\n",
    "\n",
    "# Fit Logistic Regression pipeline on the training data, get predictions and plot confusion matrix\n",
    "logreg_allgram_pipe.fit(X_allgrams, y)\n",
    "logreg_allgram_yhat = logreg_allgram_pipe.predict(X_allgrams)\n",
    "plot_confusion_matrix(logreg_allgram_pipe, X_allgrams, y, normalize='true');\n",
    "print(accuracy_score(y, logreg_allgram_yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtained cross-validated accuracy score\n",
    "logreg_allgram_cv = cross_validate(logreg_allgram_pipe, X_allgrams, y)\n",
    "print(logreg_allgram_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print cross-validated mean accuracy score & add to model performance dictionary\n",
    "print(logreg_allgram_cv['test_score'].mean())\n",
    "mod_perf['logreg_allgrams'] = logreg_allgram_cv['test_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Support Vector Machine - Stochastic Gradient Descent\n",
    "Stochastic gradient descent is another technique that is very common in natural language processing due to its efficiency. Rather than calculating the gradient at each iteration using every observation in the dataset, it selects one observation at random for each step, drastically reducing the computational requirements. It uses linear models, similar to logistic regression, but performs gradient descent in a random (Stochastic) manner. It also works very well with sparse data, which makes it a natural fit for NLP. Given the size of my dataset, efficiency is very valuable and so SGD was a natural next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the SGDC Classifier pipeline\n",
    "sgdc_pipe = ImPipeline(steps=[\n",
    "                            ('vect', TfidfVectorizer()),\n",
    "                            ('cf', SGDClassifier(random_state=42))\n",
    "                            ]\n",
    ")\n",
    "\n",
    "#Fit SGDC pipe on the training data, get predictions and plot confusion matrix\n",
    "sgdc_pipe.fit(X, y)\n",
    "sgdc_yhat = sgdc_pipe.predict(X)\n",
    "plot_confusion_matrix(sgdc_pipe, X, y, normalize='true');\n",
    "print(accuracy_score(y, sgdc_yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtained cross-validated accuracy score\n",
    "sgdc_cv = cross_validate(sgdc_pipe, X, y)\n",
    "print(sgdc_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'fit_time': array([70.02949595, 70.80230594, 70.30221891, 70.43828201, 70.43610787])\n",
    "- 'score_time': array([14.75940609, 14.66371202, 14.59122682, 14.39158106, 14.41444707])\n",
    "- 'test_score': array([0.84893944, 0.84923944, 0.85029202, 0.85031702, 0.85053785])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print cross-validated mean accuracy score & add to model performance dictionary\n",
    "print(sgdc_cv['test_score'].mean())\n",
    "mod_perf['sgdc_1grams'] = sgdc_cv['test_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using N-grams as Features \n",
    "To see if I could improve on the model's performance over using single words, I tried giving it bi-grams and tri-grams as additional features. Ultimately, the model realized a small accuracy gain of 0.0062 by showing it bi-grams and tri-grams in addition to single words. The sgd classifier did not outperform logistic regression with the settings used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Single Words & Bi-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the SGDC pipeline with cleaned text and bi-grams\n",
    "sgdc_bigrams_pipe = ImPipeline(steps=[\n",
    "                ('CT', ColumnTransformer([(x, TfidfVectorizer(), x) for x in ['text_cleaned',\n",
    "                                                                              'bigrams']\n",
    "                                         ])),\n",
    "                ('cf', SGDClassifier(random_state=42))\n",
    "]\n",
    ")\n",
    "\n",
    "# Fit SGDC pipe on the training data, get predictions and plot confusion matrix\n",
    "sgdc_bigrams_pipe.fit(X_bigrams, y)\n",
    "sgdc_bigrams_yhat = sgdc_bigrams_pipe.predict(X_bigrams)\n",
    "plot_confusion_matrix(sgdc_bigrams_pipe, X_bigrams, y, normalize='true');\n",
    "print(accuracy_score(y, sgdc_bigrams_yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgdc_bigrams_cv = cross_validate(sgdc_bigrams_pipe, X_bigrams, y)\n",
    "print(sgdc_bigrams_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print cross-validated mean accuracy score & add to model performance dictionary\n",
    "print(sgdc_bigrams_cv['test_score'].mean())\n",
    "mod_perf['sgdc_2grams'] = sgdc_bigrams_cv['test_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Single Words & Tri-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the SGDC pipeline with clean text and tri-grams\n",
    "sgdc_trigrams_pipe = ImPipeline(steps=[\n",
    "                ('CT', ColumnTransformer([(x, TfidfVectorizer(), x) for x in ['text_cleaned',\n",
    "                                                                              'trigrams']\n",
    "                                                             ])),                             \n",
    "                ('cf', SGDClassifier(random_state=42))\n",
    "]\n",
    ")\n",
    "\n",
    "# Fit SGDC pipe on the training data, get predictions and plot confusion matrix\n",
    "sgdc_trigrams_pipe.fit(X_trigrams, y)\n",
    "sgdc_trigrams_yhat = sgdc_trigrams_pipe.predict(X_trigrams)\n",
    "plot_confusion_matrix(sgdc_trigrams_pipe, X_trigrams, y, normalize='true');\n",
    "print(accuracy_score(y, sgdc_trigrams_yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgdc_trigrams_cv = cross_validate(sgdc_trigrams_pipe, X_trigrams, y)\n",
    "print(sgdc_trigrams_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print cross-validated mean accuracy score & add to model performance dictionary\n",
    "print(sgdc_trigrams_cv['test_score'].mean())\n",
    "mod_perf['sgdc_3grams'] = sgdc_trigrams_cv['test_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Single Words, Bi-Grams & Tri-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the SGDC pipeline with clean text, bi-grams and tri-grams\n",
    "sgdc_allgrams_pipe = ImPipeline(steps=[\n",
    "                ('CT', ColumnTransformer([(x, TfidfVectorizer(), x) for x in ['text_cleaned',\n",
    "                                                                              'bigrams',\n",
    "                                                                              'trigrams']\n",
    "                                                             ])),                             \n",
    "                ('cf', SGDClassifier(random_state=42))\n",
    "]\n",
    ")\n",
    "\n",
    "# Fit SGDC pipe on the training data, get predictions and plot confusion matrix\n",
    "sgdc_allgrams_pipe.fit(X_allgrams, y)\n",
    "sgdc_allgrams_yhat = sgdc_allgrams_pipe.predict(X_allgrams)\n",
    "plot_confusion_matrix(sgdc_allgrams_pipe, X_allgrams, y, normalize='true');\n",
    "print(accuracy_score(y, sgdc_allgrams_yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtained cross-validated accuracy score\n",
    "sgdc_allgrams_cv = cross_validate(sgdc_allgrams_pipe, X_allgrams, y)\n",
    "print(sgdc_allgrams_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print cross-validated mean accuracy score & add to model performance dictionary\n",
    "print(sgdc_allgrams_cv['test_score'].mean())\n",
    "mod_perf['sgdc_allgrams'] = sgdc_allgrams_cv['test_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe of model performance using the mod_perf dictionary\n",
    "mod_perf_df = pd.DataFrame.from_dict(mod_perf, orient='index')\n",
    "mod_perf_df.reset_index(inplace=True)\n",
    "mod_perf_df.columns = ['model', 'accuracy']\n",
    "\n",
    "# Save model performance to disk for future use, given long execution time for the notebook.\n",
    "mod_perf_df.to_csv('../models/model_performance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of Model Perforamnce\n",
    "\n",
    "# Read model performance from csv\n",
    "mod_perf_df = pd.read_csv('../models/model_performance.csv', index_col=0)\n",
    "\n",
    "# Set custom colorblind-friendly colors\n",
    "colors = [[86/255,180/255,233/255], [86/255,180/255,233/255], [86/255,180/255,233/255],\n",
    "         [86/255,180/255,233/255], [86/255,180/255,233/255], [86/255,180/255,233/255],\n",
    "         [0,158/255,115/255], [86/255,180/255,233/255], [86/255,180/255,233/255],\n",
    "         [86/255,180/255,233/255], [86/255,180/255,233/255], [86/255,180/255,233/255]]\n",
    "\n",
    "# Plot Bar Graph\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams.update({'font.size': 13})\n",
    "plt.axis([None, None, 0.8, 0.88])\n",
    "\n",
    "ax.bar(mod_perf_df['model'], mod_perf_df['accuracy'], color=colors)\n",
    "ax.set_title('Model Performance by Cross-validated Accuracy Score')\n",
    "ax.set_ylabel('Accuracy Score (%)', labelpad=10)\n",
    "ax.set_xlabel('Model Variety', labelpad=10)\n",
    "ax.set_yticklabels(labels = ['80', '81', '82', '83', '84', '85', '86', '87', '88'])\n",
    "ax.tick_params(axis=\"x\", rotation=90)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure to file for use in presentation\n",
    "#plt.savefig('../images/model_performance.png', dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Evaluation\n",
    "The logistic regression model that trained on a combination of 1-grams and 3-grams had the highest accuracy score, so I selected it as the appropriate model for this task. The model's accuracy was only very slightly (0.0004) better than a similarly-configured model that received 1-grams, 2-grams and 3-grams. However slight the margin, when combined with the fact that it requires significantly less data and therefore a shorter cleaning process, its superiority becomes clear.\n",
    "\n",
    "The model appears to generalize quite well, as it produced an accuracy score of 87.19% on completely unseen data, as compared to cross-validated accuracy of 87.14% on the training data. It correctly classified 88% of positive reviews and 87% of negative reviews in the test data, suggesting that it is slightly more accurate at determining positive sentiment than negative sentiment. The model avoids significant weaknesses with false positives or negatives, and significantly reduces the number of reviews that would require human review in a real-world setting. Particularly for sellers that wish to set up their own online storefront, the model’s efficiency is highly appealing.\n",
    "\n",
    "The primary weakness of my model lies in its performance relative to more sophisticated NLP algorithms. There are a number of highly accurate classifiers available, and NLP as a field is advancing rapidly. However, more advanced techniques can require compute resources that may not be available to small sellers for budgetary or technical literacy reasons. My model is small enough to be deployed in a web application for easy upload/download of customer reviews and classification. It could be improved upon by a large marketplace with the financial and technical resources to tune its hyperparameters, though it is likely that an organization with those resources could implement a more sophisticated and accurate model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Test Data\n",
    "The process below follows the same process as the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the test data\n",
    "df_test = pd.read_csv('../data/test_cleaned_10sw.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find & Display any NaN values for cleaned text \n",
    "print(df_test.isna().sum())\n",
    "df_test[df_test['text_cleaned'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the single NaN row\n",
    "df_test.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save off feature combinations as required for use in the final model.\n",
    "X_test_trigrams = df_test[['text_cleaned', 'trigrams']]\n",
    "y_test = df_test['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load & Score Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best-performing model from file\n",
    "with open('../models/sw_logreg_trigram_model.pkl', 'rb') as f:\n",
    "    logreg_trigram_pipe = joblib.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the final model on the test data & plot confusion matrix\n",
    "print(logreg_trigram_pipe.score(X_test_trigrams, y_test))\n",
    "plt.style.use('default')\n",
    "plot_confusion_matrix(logreg_trigram_pipe, X_test_trigrams, y_test, normalize='true');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions and print classification report for visual inspection\n",
    "yhat_test = logreg_trigram_pipe.predict(X_test_trigrams)\n",
    "print(classification_report(y_test, yhat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing reviews from reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the reddit data \n",
    "df_reddit = pd.read_csv('../data/reddit_cleaned.csv')\n",
    "\n",
    "# Save features to appropriate variable & visually inspect\n",
    "X_red_trigrams = df_reddit[['text_cleaned', 'trigrams']]\n",
    "X_red_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values\n",
    "df_reddit.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to the X_red_trigrams dataframe containing the model's predictions for comparison\n",
    "X_red_trigrams.insert(loc=2, column=\"sentiment\", value=pd.Series(logreg_trigram_pipe.predict(X_red_trigrams)))\n",
    "X_red_trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Reviews from women's fashion dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the fashion data\n",
    "df_fashion = pd.read_csv('../data/fashion_cleaned.csv')\n",
    "\n",
    "# Save features & target to appropriate variables & visually inspect\n",
    "X_fash_trigrams = df_fashion[['text_cleaned', 'trigrams']]\n",
    "y_fash = df_fashion['sentiment']\n",
    "X_fash_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fash_trigrams.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the final model on the fashion data & plot confusion matrix\n",
    "print(logreg_trigram_pipe.score(X_fash_trigrams, y_fash))\n",
    "plt.style.use('default')\n",
    "plot_confusion_matrix(logreg_trigram_pipe, X_fash_trigrams, y_fash, normalize='true');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions and print classification report for visual inspection\n",
    "yhat_fash = logreg_trigram_pipe.predict(X_fash_trigrams)\n",
    "print(classification_report(y_fash, yhat_fash))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The model was able to achieve solid accuracy scores and improve upon the baseline MNB model with limited tuning and a reasonable train time. The performance is thanks largely to the quality, size and balance of the dataset. I would recommend the model to businesses wishing to create their own storefronts in order to avoid rating inflation problems down the road. It has proven to be very accurate at classifying review sentiment, and it is relatively easy to identify a list of incorrect predictions with basic python that could be inspected by a human.\n",
    "\n",
    "The model is likely less suited for large-scale deployment by Amazon or a similar marketplace, as there are superior options that exist, and large marketplaces have the resources to support the computational demands of more sophisticated models. In the case of Amazon, it’s likely to be cost-efficient to develop their own classifier that is constantly training on new user reviews as they happen. \n",
    "\n",
    "### Further Development\n",
    "The next step in the project will be to refactor the cleaning & training code to be utilized in a web application. I envision the application providing a way for sellers that have previously used a storefront like Amazon or Rakuten to upload all of their review data and have it classified before leaving the site. Amazon famously does not provide an api for review data, so it will be up to the business to gather the data themselves. This functionality would help a business efficiently classify product sentiment at the time of transition, and form a baseline for their understanding to build from. \n",
    "\n",
    "From a legal perspective, a business would need written permission from each review author in order to actually transfer the reviews. That process will be horribly inefficient and likely not worth the trouble, especially for heavily-reviewed products. My tool provides a partial solution by helping businesses understand with a high degree of accuracy the proportion of their reviews that are actually positive or negative before transitioning away from the larger marketplace.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spark-cap-env)",
   "language": "python",
   "name": "spark-cap-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
