{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title\n",
    "Author: Tom Chapman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "Since their inception, online marketplaces have fundamentally shifted how consumers shop. With a near limitless number of products and services available online, it has never been less important for a consumer to walk into a brick-and-mortar store to make a purchase. However, identifying which products or services are high-quality has become increasingly difficult. The popularity of the 5-star rating scale has led to a number of well-documented challenges. [Harvard Business Review](https://hbr.org/2019/07/the-problems-with-5-star-rating-systems-and-how-to-fix-them) summed these challenges up nicely as follows:\n",
    "\n",
    "- There is little incentive for consumers to provide truthful feedback, meaning that extreme experiences (whether positive or negative) are much more likely to lead a consumer to leave a review.\n",
    "- Compounding the lack of incentive for truth, 5-star ratings are prone to grade inflation. There is no correlation between the star-rating and the sentiment the user expresses in a review. It's possible (and surprisingly common) for a user to hate a product, excorriate it in a review, and then rate it 5-stars. This leads to inflated ratings, and makes it harder for the consumer to understand the review variance between products.\n",
    "\n",
    "My tool is intended to help address rating inflation by classifying user feedback as positive or negative based on its content. I also prioritized ease-of-implementation by limiting model size and complexity wherever possible. I want this tool to be accessible to sellers without significant compute resources, so it needs to function well on a large dataset AND be runnable on a single machine. I used natural language processing for this project, AND IMPLEMENTED A TOOL CAPABLE OF INGESTING USER REVIEWS FROM XXXX \n",
    "\n",
    "Amazon is the largest online marketplace currently in existence, and its challenges with rating inflation are well-documented. My tool could be useful to help reclassify ratings based on their sentiment for Amazon itself. It is also useful for sellers that want to move away from Amazon or implement their own storefront. By implementing my algorithm in a newly-created storefront, sellers can better classify consumer feedback and derive a more accurate understanding of how their products or services are being received.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import plot_confusion_matrix, confusion_matrix, make_scorer, recall_score, accuracy_score, precision_score, f1_score, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid, GridSearchCV, cross_validate, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from imblearn.pipeline import Pipeline as ImPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "I utilized a number of sklearn's modeling techniques for the project, and performed grid searches to optimize the hyperparameters of the best-performing model. As noted in the data-preparation description, I experimented with the inclusion of bi-grams and tri-grams created from user input during the modeling process as well. Ultimately..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "df = pd.read_csv('../data/train_cleaned.csv', usecols=['sentiment', 'text_cleaned', 'toks', 'trigrams'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isna().sum())\n",
    "df[df['text_cleaned'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing text_cleaned values\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text_cleaned']\n",
    "X_bigrams = df['text_cleaned', 'bigrams']\n",
    "x_trigrams = df['text_cleaned', 'trigrams']\n",
    "x_allgrams = df['text_cleaned, bigrams', 'trigrams']\n",
    "X_grams = df['trigrams']\n",
    "y = df['sentiment']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Classifier\n",
    "To begin modelling efforts, I built a dummy classifier that simply guesses the most common class every time. Given that our training data is almost (but not perfectly) balanced, we should expect it to predict that every review is XXXX since there are slightly more XXXX reviews than YYYY reviews. If we can't beat a 50-50 guess with a dataset this large, something is horribly wrong. Predictably, the dummy classifier achieved an accuracy score of almost exactly 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Dummy Classifier pipeline\n",
    "dummy_pipe = ImPipeline(steps=[\n",
    "                                ('vect', TfidfVectorizer(max_features=max_features)),\n",
    "                                ('dc', DummyClassifier(strategy='most_frequent',\n",
    "                                                           random_state=42))\n",
    "                              ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit dummy pipe on the training data and plot confusion matrix\n",
    "dummy_pipe.fit(X, y)\n",
    "dummy_yhat = dummy_pipe.predict(X)\n",
    "plot_confusion_matrix(dummy_pipe, X, y);\n",
    "print(accuracy_score(y, dummy_yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes\n",
    "MNB is a commonly-used algorithm for natural language processing due to effectiveness in classifying topics while maintaining a low training time and relative simplicity. It predicts the probability that a given document belongs to a particular class based on the words it contains. I employed a multinomial naive bayes classifier as my first simple model for these reasons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the Multinomial Naieve Bayes Pipeline\n",
    "mnb_pipe = ImPipeline(steps=[\n",
    "                            ('vect', TfidfVectorizer(max_features=max_features)),\n",
    "                            ('mnb', MultinomialNB())\n",
    "                            ]\n",
    ")\n",
    "\n",
    "# Fit MNB pipe on the training data, get predictions and plot confusion matrix\n",
    "mnb_pipe.fit(X, y)\n",
    "mnb_yhat = mnb_pipe.predict(X)\n",
    "plot_confusion_matrix(mnb_pipe, X, y, normalize='true');\n",
    "print(accuracy_score(y, mnb_yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_pipe.named_steps['mnb'].n_features_\n",
    "#__mnb.n_features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtained cross-validated accuracy score\n",
    "cross_validate(mnb_pipe, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using N-grams as Features\n",
    "To see if I could improve on the model's performance over using single words, I tried giving it bi-grams and tri-grams as additional features. Ultimately XXXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Multinomial Naieve Bayes Pipeline\n",
    "mnb_pipe = ImPipeline(steps=[\n",
    "                            ('vect', TfidfVectorizer(max_features=max_features)),\n",
    "                            ('mnb', MultinomialNB())\n",
    "                            ]\n",
    ")\n",
    "\n",
    "# Fit MNB pipe on the training data, get predictions and plot confusion matrix\n",
    "mnb_pipe.fit(X_grams, y)\n",
    "mnb_yhat = mnb_pipe.predict(X_grams)\n",
    "plot_confusion_matrix(mnb_pipe, X_grams, y, normalize='true');\n",
    "print(accuracy_score(y, mnb_yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent Classifier\n",
    "Stochastic gradient descent is another technique that is very common in natural language processing due to its efficiency. Rather than calculating the gradient at each iteration using every observation, it selects one observation at random for each step, drastically reducing the computational requirements. It also works very well with sparse data, which makes it a natural fit for NLP. Given the size of my dataset, SGD was a natural next step and XXXX..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the SGDC Classifier pipeline\n",
    "sgdc_pipe = ImPipeline(steps=[\n",
    "                            ('vect', TfidfVectorizer(max_features=max_features,\n",
    "                                                    #max_df=0.95,\n",
    "                                                   # min_df=0.05\n",
    "                                                    )),\n",
    "                            ('sgdc', SGDClassifier(random_state=42))\n",
    "                            ]\n",
    ")\n",
    "\n",
    "#Fit SGDC pipe on the training data, get predictions and plot confusion matrix\n",
    "sgdc_pipe.fit(X, y)\n",
    "sgdc_yhat = sgdc_pipe.predict(X)\n",
    "plot_confusion_matrix(sgdc_pipe, X, y, normalize='true');\n",
    "print(accuracy_score(y, sgdc_yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtained cross-validated accuracy score\n",
    "# cross_validate(sgdc_pipe, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgdc_params = {\n",
    "            'sgdc__alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 0.0002, 0.0005],\n",
    "            #'sgdc__n_iter': [50, 100, 500],\n",
    "            'sgdc__loss': ['hinge', 'log', 'huber'],\n",
    "            'sgdc__penalty': ['l2', 'l1', 'elasticnet'],\n",
    "            'vect__max_features': [None, 50000, 100000, 200000]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sgdc_gs = GridSearchCV(sgdc_pipe, param_grid=sgdc_params, n_jobs=-2, verbose=3, cv=3)\n",
    "#sgdc_gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgdc_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the SDGC Classifier pipeline\n",
    "sgdc_tuned_pipe = ImPipeline(steps=[\n",
    "                            ('vect', TfidfVectorizer(max_features=50000,\n",
    "                                                    #max_df=0.95,\n",
    "                                                   # min_df=0.05\n",
    "                                                    )),\n",
    "                            ('sgdc', SGDClassifier(random_state=42,\n",
    "                                                  alpha=0.0001,\n",
    "                                                  loss = 'hinge',\n",
    "                                                  penalty='l2'\n",
    "                                                  ))\n",
    "                            ]\n",
    ")\n",
    "\n",
    "#Fit SGDC pipe on the training data, get predictions and plot confusion matrix\n",
    "sgdc_tuned_pipe.fit(X, y)\n",
    "sgdc_tuned_yhat = sgdc_tuned_pipe.predict(X)\n",
    "plot_confusion_matrix(sgdc_tuned_pipe, X, y, normalize='true');\n",
    "print(accuracy_score(y, sgdc_tuned_yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtained cross-validated accuracy score\n",
    "cross_validate(sgdc_pipe, X_grams, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('sgdc_pkl', 'wb') as files:\n",
    "    pickle.dump(sgdc_tuned_pipe, files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using N-grams as Features \n",
    "To see if I could improve on the model's performance over using single words, I tried giving it bi-grams and tri-grams as additional features. Ultimately XXXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the SGDC Classifier pipeline\n",
    "sgdc_ngrams_pipe = ImPipeline(steps=[\n",
    "                            ('vect', TfidfVectorizer(max_features=max_features,\n",
    "                                                    #max_df=0.95,\n",
    "                                                   # min_df=0.05\n",
    "                                                    )),\n",
    "                            ('sgdc', SGDClassifier(random_state=42))\n",
    "                            ]\n",
    ")\n",
    "\n",
    "#Fit SGDC pipe on the training data, get predictions and plot confusion matrix\n",
    "sgdc_ngrams_pipe.fit(X_grams, y)\n",
    "sgdc_ngrams_yhat = sgdc_ngrams_pipe.predict(X_grams)\n",
    "plot_confusion_matrix(sgdc_ngrams_pipe, X_grams, y, normalize='true');\n",
    "print(accuracy_score(y, sgdc_ngrams_yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtained cross-validated accuracy score\n",
    "cross_validate(sgdc_pipe, X_grams, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Random Forest Pipeline\n",
    "rf_pipe = ImPipeline(steps=[\n",
    "                            ('vect', TfidfVectorizer(max_features=max_features)),\n",
    "                            ('rf', RandomForestClassifier())\n",
    "                            ]\n",
    ")\n",
    "\n",
    "# Fit RF pipe on the training data, get predictions and plot confusion matrix\n",
    "# rf_pipe.fit(X, y)\n",
    "rf_yhat = rf_pipe.predict(X)\n",
    "plot_confusion_matrix(rf_pipe, X, y);\n",
    "print(accuracy_score(y, rf_yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtained cross-validated accuracy score\n",
    "# cross_validate(rf_pipe, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('rf_pkl', 'wb') as files:\n",
    "    pickle.dump(rf_pipe, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the gradient boosting classifier pipeline\n",
    "gbc_pipe = ImPipeline(steps=[\n",
    "                                 ('vect', TfidfVectorizer(max_features=max_features)),\n",
    "                                 ('gbc',  GradientBoostingClassifier())\n",
    "                                ]\n",
    ")\n",
    "\n",
    "#Fit SGDC pipe on the training data, get predictions and plot confusion matrix\n",
    "gbc_pipe.fit(X, y)\n",
    "gbc_yhat = gbc_pipe.predict(X)\n",
    "plot_confusion_matrix(gbc_pipe, X, y);\n",
    "print(accuracy_score(y, gbc_yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtained cross-validated accuracy score\n",
    "cross_validate(gbc_pipe, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the gradient boosting classifier pipeline\n",
    "gbc_ngrams_pipe = ImPipeline(steps=[\n",
    "                                 ('vect', TfidfVectorizer(max_features=max_features)),\n",
    "                                 ('gbc',  GradientBoostingClassifier())\n",
    "                                ]\n",
    ")\n",
    "\n",
    "#Fit SGDC pipe on the training data, get predictions and plot confusion matrix\n",
    "gbc_ngrams_pipe.fit(X_grams, y)\n",
    "gbc_ngrams_yhat = gbc_ngrams_pipe.predict(X_grams)\n",
    "plot_confusion_matrix(gbc_ngrams_pipe, X_grams, y);\n",
    "print(accuracy_score(y, gbc_yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtained cross-validated accuracy score\n",
    "cross_validate(gbc_pipe, X_grams, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Evaluation\n",
    "The XXXX model ended up performing best on the test data, so I selected it as the appropriate model to deploy. \n",
    "\n",
    "- Score\n",
    "- what did it do well\n",
    "- what are its shortcomings\n",
    "- how well does it solve the business problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spark-cap-env)",
   "language": "python",
   "name": "spark-cap-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
